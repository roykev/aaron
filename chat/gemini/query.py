"""
RAG query functionality for Gemini File Search
"""

import time
import re
import os
import glob
from google import genai
from google.genai import types
from typing import List, Optional, Dict
from query_logger import QueryLogger
from chunk_matcher import ChunkMatcher


class QueryEngine:
    """Handles RAG queries against the file search store"""

    def __init__(
        self,
        client: genai.Client,
        store_name: str,
        topics: str,
        summary: str,
        model_name: str = "gemini-2.0-flash-exp",
        max_files: int = 10,
        logger: Optional[QueryLogger] = None,
        class_level: Optional[str] = None,
        query_scope: str = "course",
        lecture_id: Optional[str] = None,
        top_chunks_to_show: int = 3
    ):
        """
        Initialize query engine

        Args:
            client: Gemini API client
            store_name: Name of the file search store
            topics: Course topics string
            summary: Lecture summary
            model_name: Gemini model to use
            max_files: Maximum number of files to use as context
            logger: QueryLogger instance for logging queries (optional)
            class_level: Class level for filtering (used when scope is 'class')
            query_scope: Scope of queries - 'class' (single class) or 'course' (entire course)
            lecture_id: Specific lecture ID for class-level filtering (e.g., 'psychology_20251206_234701')
            top_chunks_to_show: Number of top chunks to display with content (default: 3)
        """
        self.client = client
        self.store_name = store_name
        self.topics = topics
        self.summary = summary
        self.model_name = model_name
        self.max_files = max_files
        self.logger = logger
        self.class_level = class_level
        self.query_scope = query_scope
        self.lecture_id = lecture_id
        self.top_chunks_to_show = top_chunks_to_show

    def _filter_files_by_scope(self, files: List) -> List:
        """
        Filter files based on query scope (class vs course)

        Args:
            files: List of all files

        Returns:
            Filtered list of files based on scope
        """
        if self.query_scope == "course":
            # Course scope: use all files in the store
            return files
        elif self.query_scope == "class" and self.class_level:
            # Class scope: filter by class_level in filename
            # Assuming filenames contain class level info like: business_grad1_Lec1_chunk_...
            # For now, we'll use a simple approach - if class_level is specified,
            # filter files that contain the class identifier in their name
            class_identifier = self.class_level.replace(" ", "_").lower()
            filtered = [f for f in files if class_identifier in f.name.lower()]
            if filtered:
                return filtered
            # If no files match, fall back to all files
            return files
        else:
            # Default: use all files
            return files

    def _extract_chunk_references(self, files_used: List) -> List[Dict]:
        """
        Extract time chunk references from file names and read their content

        Args:
            files_used: List of file objects used in the query

        Returns:
            List of chunk references with lecture_id, time range, and content
        """
        chunks = []
        for file in files_used:
            # File names are expected to be in format: LectureID_START-END.txt
            # Example: PsychLec4_00-00-00_to_00-00-30.txt
            match = re.match(r'(.+?)_(\d{2}-\d{2}-\d{2})_to_(\d{2}-\d{2}-\d{2})\.txt', file.name)
            if match:
                lecture_id = match.group(1)
                start_time = match.group(2).replace('-', ':')
                end_time = match.group(3).replace('-', ':')

                # Try to read the chunk content from the file
                try:
                    # Download file content
                    file_content = self.client.files.get(name=file.name)
                    content = file_content.get('content', 'Content not available')
                except Exception as e:
                    content = f"Error reading file: {e}"

                chunks.append({
                    "lecture_id": lecture_id,
                    "start_time": start_time,
                    "end_time": end_time,
                    "chunk_file": file.name,
                    "content": content
                })
        return chunks

    def query(self, user_query: str, verbose: bool = True) -> dict:
        """
        Perform RAG query with timing, logging, and chunk references

        Args:
            user_query: User's question
            verbose: Whether to print detailed output

        Returns:
            Dictionary with answer, metadata, timing, and chunk references
        """
        # Start timing
        start_time = time.time()

        if verbose:
            print(f"\n-> Running query with store reference: {self.store_name}")

        # Create system instruction with context
        system_instruction = (
            "You are an expert teaching assistant for an Advanced Psychology course, specifically the 'Memory' unit. "
            "Your responses MUST be grounded ONLY in the provided source material. "
            f"Course Topics: {self.topics}\n"
            f"Lecture Summary: {self.summary}\n"
        )

        # Check store document count
        store = self.client.file_search_stores.get(name=self.store_name)
        active_docs = store.active_documents_count or 0

        if verbose:
            print(f"Store has {active_docs} active documents")

        if active_docs == 0:
            if verbose:
                print("âš ï¸  Warning: Store has no documents. Upload files using batch.py first.")
            # Return empty result
            return {
                'query': user_query,
                'answer': "No documents found in store. Please upload lecture content first.",
                'model': self.model_name,
                'store': self.store_name,
                'response_time_seconds': 0,
                'chunks_used': []
            }

        # Configure File Search tool with store
        # Build metadata filter for class-level filtering if needed
        metadata_filter = None
        if self.query_scope == "class" and self.lecture_id:
            # Filter to specific lecture ID for class-level queries
            metadata_filter = types.MetadataFilter(
                key="lecture_id",
                value=self.lecture_id,
                match_type="EXACT"
            )
            if verbose:
                print(f"-> Using class scope filter: lecture_id={self.lecture_id}")

        # Configure File Search tool
        file_search_config = types.FileSearch(
            file_search_store_names=[self.store_name]
        )

        # Add metadata filter if applicable
        if metadata_filter:
            file_search_config = types.FileSearch(
                file_search_store_names=[self.store_name],
                metadata_filter=metadata_filter
            )

        # Create tool configuration
        tools = [types.Tool(file_search=file_search_config)]

        if verbose:
            print(f"-> Query scope: {self.query_scope}")

        # Generate response with File Search Store
        response = self.client.models.generate_content(
            model=self.model_name,
            contents=user_query,
            config=types.GenerateContentConfig(
                system_instruction=system_instruction,
                tools=tools
            )
        )

        # Calculate response time
        response_time = time.time() - start_time

        # Try to extract grounding metadata first (preferred method)
        chunks_used = []
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                grounding = candidate.grounding_metadata

                if verbose:
                    print(f"\n-> Grounding metadata available")
                    if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                        print(f"   Found {len(grounding.grounding_chunks)} grounding chunks")

                # Extract chunk information from grounding metadata
                if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                    for i, chunk in enumerate(grounding.grounding_chunks):
                        chunk_info = {
                            'chunk_index': i,
                            'content': 'Grounding chunk from File Search Store'
                        }

                        # Try to extract chunk details if available
                        if hasattr(chunk, 'retrieved_context'):
                            ctx = chunk.retrieved_context
                            if hasattr(ctx, 'uri'):
                                chunk_info['uri'] = ctx.uri
                            if hasattr(ctx, 'title'):
                                chunk_info['title'] = ctx.title
                            if hasattr(ctx, 'text'):
                                chunk_info['content'] = ctx.text

                        chunks_used.append(chunk_info)

        # Fallback: Use answer-to-chunk similarity matching if no grounding metadata
        if not chunks_used and verbose:
            print("\n-> No grounding metadata - using answer similarity matching")

            # Find chunks directory based on scope
            chunks_dirs = glob.glob(os.path.join(os.getcwd(), "*_chunks"))

            if self.query_scope == "class" and self.lecture_id:
                # Class scope: use specific lecture's chunks directory
                lecture_chunks_dir = f"{self.lecture_id}_chunks"
                if os.path.exists(lecture_chunks_dir):
                    chunks_dir = lecture_chunks_dir
                    if verbose:
                        print(f"-> Using class-level chunks: {lecture_chunks_dir}")
                elif chunks_dirs:
                    # Fallback: find chunks dir matching lecture_id
                    matching_dirs = [d for d in chunks_dirs if self.lecture_id in d]
                    if matching_dirs:
                        chunks_dir = matching_dirs[0]
                        if verbose:
                            print(f"-> Using class-level chunks: {chunks_dir}")
                    else:
                        chunks_dir = max(chunks_dirs, key=os.path.getmtime)
                        if verbose:
                            print(f"-> Warning: No chunks found for {self.lecture_id}, using most recent")
                else:
                    chunks_dir = None
            elif chunks_dirs:
                # Course scope: use most recent chunks directory (contains all lectures)
                chunks_dir = max(chunks_dirs, key=os.path.getmtime)
                if verbose:
                    print(f"-> Using course-level chunks: {chunks_dir}")
            else:
                chunks_dir = None

            if chunks_dir:
                # Create chunk matcher and find similar chunks
                matcher = ChunkMatcher(chunks_dir)

                # For class scope, filter matches to specific lecture_id
                if self.query_scope == "class" and self.lecture_id:
                    all_matches = matcher.find_matching_chunks(response.text, top_k=self.top_chunks_to_show * 3)
                    # Filter to only this lecture_id
                    matches = [m for m in all_matches if m['lecture_id'] == self.lecture_id][:self.top_chunks_to_show]
                else:
                    matches = matcher.find_matching_chunks(response.text, top_k=self.top_chunks_to_show)

                # Convert to chunks_used format
                chunks_used = [
                    {
                        'lecture_id': m['lecture_id'],
                        'start_time': m['start_time'],
                        'end_time': m['end_time'],
                        'filename': m['filename'],
                        'content': m['content'],
                        'similarity': m['similarity']
                    }
                    for m in matches
                ]

                if verbose and chunks_used:
                    print(f"   Found {len(chunks_used)} matching chunks based on answer similarity")

        # Prepare result
        result = {
            'query': user_query,
            'answer': response.text,
            'model': self.model_name,
            'store': self.store_name,
            'response_time_seconds': round(response_time, 2),
            'chunks_used': chunks_used
        }

        # Log the query if logger is available
        if self.logger:
            self.logger.log_query(
                query=user_query,
                answer=response.text,
                model=self.model_name,
                store=self.store_name,
                response_time_seconds=response_time,
                chunks_used=chunks_used
            )

        if verbose:
            print("-" * 70)
            print("ðŸ¤– Model Response:")
            print("-" * 70)
            print(response.text)
            print("-" * 70)
            print(f"â±ï¸  Response time: {response_time:.2f} seconds")

            if chunks_used:
                print(f"\nðŸ“š Source Chunks ({len(chunks_used)} total):")
                print("=" * 70)
                for i, chunk in enumerate(chunks_used[:self.top_chunks_to_show], 1):
                    # Check if this is a similarity-matched chunk or grounding metadata chunk
                    if 'lecture_id' in chunk and 'start_time' in chunk:
                        # Similarity-matched chunk with timestamps
                        print(f"\n[{i}] {chunk['lecture_id']}")
                        print(f"    â° Time: {chunk['start_time']} - {chunk['end_time']}")
                        if 'filename' in chunk:
                            print(f"    ðŸ“„ File: {chunk['filename']}")
                        if 'similarity' in chunk:
                            print(f"    ðŸŽ¯ Similarity: {chunk['similarity']:.3f}")
                    else:
                        # Grounding metadata chunk
                        print(f"\n[{i}] Chunk {chunk.get('chunk_index', i)}")
                        if 'title' in chunk:
                            print(f"    ðŸ“„ Title: {chunk['title']}")
                        if 'uri' in chunk:
                            print(f"    ðŸ”— URI: {chunk['uri']}")

                    # Display chunk content preview
                    content = chunk.get('content', 'Content not available')
                    if content and content != 'Content not available':
                        # Truncate if too long
                        max_content_length = 200
                        if len(content) > max_content_length:
                            content_preview = content[:max_content_length] + "..."
                        else:
                            content_preview = content
                        print(f"    ðŸ“ Content:\n       {content_preview.replace(chr(10), chr(10) + '       ')}")
                    else:
                        print(f"    ðŸ“ Content: {content}")
                    print()

                if len(chunks_used) > self.top_chunks_to_show:
                    print(f"   ... and {len(chunks_used) - self.top_chunks_to_show} more chunks used")
            print("-" * 70)

        return result

    def search_and_answer(self, query: str) -> dict:
        """
        Perform search and return structured answer

        Args:
            query: User's search query

        Returns:
            Dictionary with 'answer' and metadata
        """
        # Query method now returns a dict with all metadata
        return self.query(query, verbose=False)

    def batch_query(self, queries: List[str]) -> List[dict]:
        """
        Process multiple queries in batch

        Args:
            queries: List of user queries

        Returns:
            List of answer dictionaries
        """
        results = []
        for i, query in enumerate(queries, 1):
            print(f"\n[Query {i}/{len(queries)}]: {query}")
            result = self.search_and_answer(query)
            results.append(result)
            print(f"Answer: {result['answer'][:200]}...")  # Print first 200 chars

        return results